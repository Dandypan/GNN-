from sklearn.cluster import KMeans
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATv2Conv, SAGEConv, GINConv
from torch_geometric.data import Data
from sklearn.metrics import accuracy_score, f1_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore")


class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()



def load_email_communication_network(file_path):
    edges = pd.read_csv(file_path, sep='\t', comment='#', header=None, names=['sender', 'receiver'])
    edges.dropna(subset=['sender', 'receiver'], inplace=True)
    edges['sender'] = edges['sender'].astype(int)
    edges['receiver'] = edges['receiver'].astype(int)

    senders = set(edges['sender'])
    receivers = set(edges['receiver'])
    sender_to_idx = {sender: idx for idx, sender in enumerate(senders)}
    receiver_to_idx = {receiver: idx + len(senders) for idx, receiver in enumerate(receivers)}

    edge_index = []
    for _, row in edges.iterrows():
        u = sender_to_idx[row['sender']]
        v = receiver_to_idx[row['receiver']]
        edge_index.append([u, v])
        edge_index.append([v, u])

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    num_nodes = len(senders) + len(receivers)

    
    features = torch.eye(num_nodes)  # Initialize as identity matrix
    data = Data(edge_index=edge_index, x=features)
    return data



def update_node_features(data, num_clusters):
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    pseudo_labels = kmeans.fit_predict(data.x.cpu().numpy())
    new_features = np.eye(num_clusters)[pseudo_labels]  # One-hot encoding
    data.x = torch.tensor(new_features, dtype=torch.float)
    data.pseudo_labels = torch.tensor(pseudo_labels, dtype=torch.long)



class HybridGNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, use_gat=True, use_sage=True, use_gin=True, gat_heads=8, dropout=0.5):
        super(HybridGNNModel, self).__init__()
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        self.residual_projs = nn.ModuleList()  # Projection layers for residuals

        current_dim = input_dim

        
        if use_gat:
            self.convs.append(GATv2Conv(current_dim, hidden_dim // gat_heads, heads=gat_heads, dropout=dropout))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))
            self.residual_projs.append(nn.Linear(current_dim, hidden_dim))
            current_dim = hidden_dim

        
        for _ in range(num_layers - 2):
            if use_sage:
                self.convs.append(SAGEConv(current_dim, hidden_dim))
                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))
                self.residual_projs.append(nn.Linear(current_dim, hidden_dim) if current_dim != hidden_dim else nn.Identity())
                current_dim = hidden_dim

        
        if use_gin:
            self.convs.append(GINConv(nn.Linear(current_dim, hidden_dim)))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))
            self.residual_projs.append(nn.Linear(current_dim, hidden_dim) if current_dim != hidden_dim else nn.Identity())
            current_dim = hidden_dim

        
        self.fc = nn.Linear(current_dim, output_dim)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        for i in range(len(self.convs)):
            residual = self.residual_projs[i](x)
            x = self.convs[i](x, edge_index)
            x = self.batch_norms[i](x)
            x = F.relu(x) + residual
            x = self.dropout(x)
        return self.fc(x)



def train_gnn_model_with_regularization(model, data, epochs=60, lr=0.01, weight_decay=1e-4):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = FocalLoss()

    accuracies, f1_scores = [], []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        out = model(data)
        loss = criterion(out, data.pseudo_labels)
        loss.backward()
        optimizer.step()

        
        model.eval()
        with torch.no_grad():
            preds = out.argmax(dim=1)
            acc = accuracy_score(data.pseudo_labels.cpu(), preds.cpu())
            f1 = f1_score(data.pseudo_labels.cpu(), preds.cpu(), average="macro")
            accuracies.append(acc)
            f1_scores.append(f1)

        print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {acc:.4f}, F1 Score: {f1:.4f}")

    return accuracies, f1_scores



if __name__ == "__main__":
    file_path = "/content/drive/MyDrive/email-Enron/email-Enron.txt"
    data = load_email_communication_network(file_path)

   
    num_clusters = 4  # Set to 4 pseudo-labels
    update_node_features(data, num_clusters)

    
    model = HybridGNNModel(input_dim=num_clusters, hidden_dim=128, output_dim=num_clusters, num_layers=6)
    accuracies, f1_scores = train_gnn_model_with_regularization(model, data, epochs=60)
